{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from random import shuffle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr_documents = 5\n",
    "relevances_cats = ['N', 'R', 'HR']\n",
    "rel_values = [0.0, 1.0, 2.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_list(a_list):\n",
    "    half = len(a_list)/2\n",
    "    return [a_list[:half], a_list[half:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_combinations_list(relevances):  \n",
    "    combinations = []\n",
    "    for i in itertools.product(relevances, repeat = nr_documents*2):\n",
    "        i = list(i)\n",
    "        i = split_list(i)\n",
    "        combinations.append(i)\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinations_cats = get_combinations_list(relevances_cats)\n",
    "shuffle(combinations_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "Calculates precision at rank k with a list with 3 relevance levels (R, HR and N). 'Precision at rank k' though, asks for a binary classication problem, so HR and R is counted as relevant (1) and N as non-relevant(0).\n",
    "\n",
    "k must be 5 or smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at(k, pairs):\n",
    "    count_TP = 0 # amount of true positives\n",
    "    count_FP = 0 # amount of false positives\n",
    "    precision_list =[]\n",
    "    for pair in pairs:\n",
    "        for m in range(0, k):\n",
    "            l = pair[0][m]\n",
    "            if l == 'R': count_TP += 1\n",
    "            elif l == 'HR': count_TP += 1\n",
    "            else : count_FP+=1\n",
    "            precision_P = count_TP / float(count_TP + count_FP)\n",
    "        count_TP = 0\n",
    "        count_FP = 0\n",
    "        for m in range(0,k):\n",
    "            l = pair[1][m]\n",
    "            if l == 'R': count_TP += 1\n",
    "            elif l == 'HR': count_TP += 1\n",
    "            else : count_FP += 1\n",
    "            precision_E = count_TP / float(count_TP + count_FP)\n",
    "        precisions = [precision_P, precision_E]\n",
    "        precision_list.append(precisions)\n",
    "    return precision_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DCG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DCG is used to get rid of the pairs where E does not outperform P. This new set of pairs is used for all other offline and online evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dcg_at(k, pairs):\n",
    "    EP_results = []\n",
    "    new_pairs_set = []\n",
    "    for pair in pairs:\n",
    "        rank_dcgs = []\n",
    "        for ranking in pair:\n",
    "            dcg = 0\n",
    "            for r in range(1,k+1): # r = 1,2,3,4,5\n",
    "                if ranking[r-1] == relevances_cats[0]:\n",
    "                    rel_value = rel_values[0]\n",
    "                elif ranking[r-1] == relevances_cats[1]:\n",
    "                    rel_value = rel_values[1]\n",
    "                else:\n",
    "                    rel_value = rel_values[2]\n",
    "                dcg += ((2**rel_value)-1)/(math.log(1+r,2))\n",
    "            rank_dcgs.append(dcg)\n",
    "            \n",
    "        if rank_dcgs[1] > rank_dcgs[0]: # if E > P\n",
    "            new_pairs_set.append(pair)\n",
    "            EP_results.append(rank_dcgs)\n",
    "    return EP_results[:2000], new_pairs_set[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ERR**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def R(g): # mapping from relevance grades g to probability of relevance  \n",
    "    if g == relevances_cats[0]:\n",
    "        rel_value = rel_values[0]\n",
    "    elif g == relevances_cats[1]:\n",
    "        rel_value = rel_values[1]\n",
    "    else:\n",
    "        rel_value = rel_values[2]\n",
    "        \n",
    "    return ((2**rel_value)-1)/float((2**max(rel_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def P(seq, r): # probability that user stops at position r\n",
    "    P = 1\n",
    "    for i in range(1,(r-1)+1):\n",
    "        P *= (1-R(seq[i])) * R(seq[r-1])\n",
    "    return P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def err(pairs): # a cascade based metric with x(r) = 1/r\n",
    "    ERR_results = []\n",
    "    for pair in pairs:\n",
    "        rank_err = []\n",
    "        for ranking in pair:\n",
    "            err = 0\n",
    "            for r in range(1, len(ranking)+1):\n",
    "                err += (1/float(r))*P(ranking, r)\n",
    "            rank_err.append(err)\n",
    "        ERR_results.append(rank_err)\n",
    "    return ERR_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_differences(results):\n",
    "    difference_measures=[]\n",
    "    for algo in results:\n",
    "        a = algo[0]\n",
    "        b = algo[1]\n",
    "        difference = b - a\n",
    "        difference_measures.append(difference)\n",
    "    return difference_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dcg, pairs_set = dcg_at(5, combinations_cats) # 59000 pairs\n",
    "\n",
    "\n",
    "precision = precision_at(5, pairs_set) # 2000 pairs\n",
    "err = err(pairs_set)\n",
    "\n",
    "\n",
    "\n",
    "difference_measures_dcg = calculate_differences(dcg)\n",
    "difference_measures_prec = calculate_differences(precision)\n",
    "difference_measures_err = calculate_differences(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_coin():\n",
    "    random.seed()\n",
    "    return random.getrandbits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates a list with 10 random bits. 1 represents a click.\n",
    "\n",
    "def generate_random_clicks():\n",
    "    clicks = []\n",
    "    for i in range(nr_documents*2):\n",
    "        clicks.append(flip_coin())\n",
    "    return clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team-draft interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def team_draft_interleaving(rankings):\n",
    "    random.seed()\n",
    "    new_ranking = []\n",
    "    for i in range(nr_documents):\n",
    "        winner = flip_coin()\n",
    "        new_ranking.append([rankings[winner][i], winner])       \n",
    "        new_ranking.append([rankings[1-winner][i], 1-winner])\n",
    "       \n",
    "    return new_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_softmaxes(rankings, tau):\n",
    "    denominator = 0\n",
    "    for i in range(1, (len(rankings[0])+1)):\n",
    "        denominator += 1 / float(i ** tau)\n",
    "    index = 1\n",
    "    softmax1 = []\n",
    "    softmax2 = []\n",
    "    for ranking in rankings[0]:\n",
    "        prob = (1/float(index**tau))/float(denominator)\n",
    "        softmax1.append(prob)\n",
    "        softmax2.append(prob)\n",
    "        index += 1\n",
    "    return softmax1, softmax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recalculate_softmax(softmax, pick):\n",
    "    softmax.remove(softmax[pick])\n",
    "    softmax[:] = [x/float(sum(softmax)) for x in softmax]\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probabilistic_interleaving(rankings1, tau):\n",
    "    probrankings = copy.deepcopy(rankings1)\n",
    "    conc_list = []\n",
    "    s1, s2 = init_softmaxes(probrankings, tau)\n",
    "    \n",
    "    while probrankings[0] or probrankings[1]:\n",
    "        \n",
    "        winner = flip_coin() # keep flipping until all lists are empty\n",
    "        \n",
    "        if winner == 0 and probrankings[0]:\n",
    "            \n",
    "            pick = np.random.choice(len(probrankings[0]), 1, p=s1)\n",
    "            conc_list.append([probrankings[winner][pick], winner]) # add pick to the concatenated list\n",
    "            probrankings[winner].remove(probrankings[winner][pick]) # remove the pick from the document list\n",
    "            s1 = recalculate_softmax(s1, pick) # recalculate the softmax of that list to normalise\n",
    "                \n",
    "        elif winner == 1 and probrankings[1]:\n",
    "            \n",
    "            pick = np.random.choice(len(probrankings[1]), 1, p=s2)\n",
    "            conc_list.append([probrankings[winner][pick], winner]) # add pick to the concatenated list\n",
    "            probrankings[winner].remove(probrankings[winner][pick]) # remove the pick from the document list\n",
    "            s2 = recalculate_softmax(s2, pick) # recalculate the softmax of that list to normalise\n",
    "                \n",
    "    return conc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_interleaf_credits(ranking, clicks):\n",
    "    credits = [0,0]\n",
    "    for i in range(len(clicks)):\n",
    "        if clicks[i] == 1:\n",
    "            credits[ranking[i][1]] += 1\n",
    "    return credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:  \n",
    "        f = f.readlines()\n",
    "        for line in f:\n",
    "            data.append(line.split())\n",
    "    return data\n",
    "\n",
    "data = process_data(\"training_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand():\n",
    "    return random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required method (c)\n",
    "def is_clicked(P):\n",
    "    result = 1 if rand() < P else 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required method (b)\n",
    "def predict_click_probabilities_RCM(nr_clicks, nr_docs):\n",
    "    return nr_clicks / float(nr_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated with the assumption that people do not see the same query-page pair more than once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required method (a)\n",
    "def get_parameter_RCM():\n",
    "    nr_clicks = 0\n",
    "    nr_doc_pages = 0\n",
    "    for row in data:\n",
    "        if 'C' in row:\n",
    "            nr_clicks += 1\n",
    "        else:\n",
    "            nr_doc_pages += 1\n",
    "    nr_docs = nr_doc_pages*10\n",
    "    return nr_clicks, nr_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nr_clicks, nr_docs = get_parameter_RCM()\n",
    "click_probability_RCM = predict_click_probabilities_RCM(nr_clicks, nr_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'RCM click probability: ', click_probability_RCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# printing the simulated clicks\n",
    "def get_clicks_RCM(documents):\n",
    "    sim_clicks_RCM = []   \n",
    "    \n",
    "    for document in range(len(documents)):\n",
    "        sim_clicks_RCM.append(is_clicked(click_probability_RCM))\n",
    "    return sim_clicks_RCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position-based model PBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_click_probabilities_PBM(documents, g):\n",
    "    click_probabilities = []\n",
    "    \n",
    "    alphas = [0] * (nr_documents*2)\n",
    "    for i in range(len(documents)):\n",
    "        if documents[i][0] == relevances_cats[0]:\n",
    "            alphas[i] = 0.2\n",
    "        if documents[i][0] == relevances_cats[1]:\n",
    "            alphas[i] = 0.8\n",
    "        if documents[i][0] == relevances_cats[-1]:\n",
    "            alphas[i] = 0.95\n",
    "\n",
    "    for i in range(nr_documents*2):\n",
    "        P = alphas[i] * g[i]\n",
    "        click_probabilities.append(P)\n",
    "        \n",
    "    return click_probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_gammas(g, a, queries):\n",
    "    all_query_gammas = [0] * 50 \n",
    "    for values in queries.values(): #[[docid, boolean],[...,...]] \n",
    "        query_gammas = g[:] # [0.5, 0.5, ...]\n",
    "        index = 0\n",
    "        for doc in values: # [docid, boolean]  \n",
    "            clicked = doc[1] # boolean\n",
    "            gamma = clicked + ((1-clicked)*(((1-a[index])*query_gammas[index])/(1-query_gammas[index]*a[index])))\n",
    "            query_gammas[index] = gamma\n",
    "            index +=1   \n",
    "        for i in range(len(query_gammas)):\n",
    "            all_query_gammas[i] += query_gammas[i]\n",
    "    \n",
    "    for i in range(len(all_query_gammas)):\n",
    "        all_query_gammas[i] /= len(queries)\n",
    "        \n",
    "    return all_query_gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_click_results(session):\n",
    "    query_results = {}\n",
    "    last_query = 0\n",
    "    for row in session:\n",
    "        # query action\n",
    "        if 'Q' in row: \n",
    "            last_query = row[3]\n",
    "            retrieved_docs = row[5:]\n",
    "            for docid in range(len((retrieved_docs))):\n",
    "                retrieved_docs[docid] = [retrieved_docs[docid], 0]\n",
    "            if row[3] not in query_results:     \n",
    "                query_results[row[3]] = retrieved_docs\n",
    "            else:\n",
    "                for i in range(len(retrieved_docs)):\n",
    "                    exists = False\n",
    "                    for document in query_results[row[3]]:\n",
    "                        if retrieved_docs[i][0] in document:\n",
    "                            exists = True\n",
    "                            break\n",
    "                    if not exists:\n",
    "                        query_results[row[3]] += [retrieved_docs[i]]\n",
    "                        \n",
    "        # click action\n",
    "        else:\n",
    "            found = False\n",
    "            while not found:\n",
    "                # check if its in the last query (most likely the correct query page)\n",
    "                for values in query_results[last_query]:\n",
    "                    if row[3] == values[0]:\n",
    "                        values[1] = 1\n",
    "                        found = True\n",
    "                        \n",
    "                # otherwise, check in other query pages\n",
    "                for queries in query_results.values():\n",
    "                    for values in queries:\n",
    "                        \n",
    "                        if row[3] == values[0]:\n",
    "                            values[1] = 1\n",
    "                            found = True\n",
    "                            \n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# required method (a)\n",
    "def get_parameters_PBM(data_slice):\n",
    "    alphas = [0.2] * 50 \n",
    "    \n",
    "    gammas = [0.6] * 50\n",
    "    learned_gammas = [0] * 50\n",
    "    \n",
    "    # get examination probabilities\n",
    "    sessions = set(map(lambda x:x[0], data_slice))\n",
    "    sessions_data = [[y for y in data_slice if y[0]==x] for x in sessions]\n",
    "    session_nr = 1\n",
    "    for session in sessions_data:\n",
    "        session_nr += 1\n",
    "        query_results = get_click_results(session)\n",
    "        session_gammas = new_gammas(gammas, alphas, query_results)\n",
    "        for i in range(len(session_gammas)):\n",
    "            learned_gammas[i] += session_gammas[i]\n",
    "    for i in range(len(learned_gammas)):\n",
    "        learned_gammas[i] /= len(sessions_data)\n",
    "        learned_gammas[i] = learned_gammas[i]\n",
    "    return learned_gammas[:nr_documents*2]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_slice = data[0:20000]\n",
    "gammas = get_parameters_PBM(data_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_clicks_PBM(documents):\n",
    "    sim_clicks_PBM = []\n",
    "    \n",
    "    click_probabilities_PBM = predict_click_probabilities_PBM(documents, gammas)\n",
    "\n",
    "    for document_index in range(len(documents)):\n",
    "        sim_clicks_PBM.append(is_clicked(click_probabilities_PBM[document_index]))\n",
    "\n",
    "    return sim_clicks_PBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 6 \n",
    "\n",
    "Experiments.\n",
    "\n",
    "\n",
    "This method runs N simulations for every E-P pair in the given set. Per simulation both a team draft and probabilistic interleaf ranking is created. Both P and E are credited through the random click model and the position-based model which simulate the clicks.\n",
    "\n",
    "It returns 4 proportions of wins of E: team-RCM, team-PBM, prob-RCM, prob-PBM\n",
    "\n",
    "We meassure the proportion E by:\n",
    "\n",
    "(nr. wins of E + 0.5* nr. ties) / nr. pairs\n",
    "\n",
    "So for a tie, the credits are divided for P and E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def interleaving_experiment(ranking_pairs, N):\n",
    "    print 'p_messures: [team-RCM, team-PBM, prob-RCM, prob-PBM]'\n",
    "    all_p_meassures = []\n",
    "    nr_pair = 0\n",
    "    for pair in ranking_pairs:\n",
    "        total_games = [[0,0],[0,0]]\n",
    "        p_meassures = [[0,0],[0,0]]\n",
    "        nr_pair += 1\n",
    "        \n",
    "\n",
    "        # executing the N simulations\n",
    "        for i in range(N):\n",
    "            \n",
    "            interleaf_rankings = [team_draft_interleaving(pair), probabilistic_interleaving(pair, 3)]\n",
    "            \n",
    "            interleaf_algo_nr = 0\n",
    "            for ranking in interleaf_rankings:\n",
    "                \n",
    "                \n",
    "                # generating 2 sets of clicks of the models          \n",
    "                click_models = [get_clicks_RCM(ranking), get_clicks_PBM(ranking)]\n",
    "                \n",
    "                click_algo_nr = 0\n",
    "                for clicks in click_models:\n",
    "                    credits = get_interleaf_credits(ranking, clicks)\n",
    "                    total_games[interleaf_algo_nr][click_algo_nr] += 1\n",
    "                    if credits[0] == credits[1]: # tie\n",
    "                        p_meassures[interleaf_algo_nr][click_algo_nr] += 0.5\n",
    "                    else: # if not a tie:\n",
    "                        winner = credits.index(max(credits)) # 0 or 1: P or E\n",
    "                        if winner == 1:\n",
    "                            p_meassures[interleaf_algo_nr][click_algo_nr] += 1\n",
    "                    click_algo_nr += 1\n",
    "                interleaf_algo_nr += 1\n",
    "                            \n",
    "        # dividing the E-win-counts by the total nr of games to get the P meassure                        \n",
    "        for i in range(len(p_meassures)): # 0 and 1\n",
    "            for j in range(len(p_meassures[i])): # \n",
    "                p_meassures[i][j] /= float(N)\n",
    "                \n",
    "        \n",
    "        p_meassures = reduce(lambda x, y: x + y, p_meassures, [])        \n",
    "        print 'Pair: ', nr_pair, '/', len(ranking_pairs), p_meassures\n",
    "        \n",
    "        all_p_meassures.append(p_meassures)\n",
    "        \n",
    "    # printing the results\n",
    "    return all_p_meassures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_p_meassures = interleaving_experiment(pairs_set, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**STEP 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The production algorithm P and the experimenal algorithm E are compared by several evaluations. In this section we compare the results of the offline evaluations (delta meassures) with the results of the online evaluations (i.e. proportion of wins).  They all attempt to meassure the \"happiness\" of a potential user. We analyze them and reach our conclusions regarding their agreement.\n",
    "\n",
    "**Offline Evaluations**\n",
    "\n",
    "For the offline evaluations we used the three meassurements: (1) Precision, (2) Discounted Cumulative Gain (DCG) and (3) Expected Reciprocal Rank (ERR).\n",
    "\n",
    "Precision is a traditional evaluation meassure. It considers the relevance as a binary classification problem. Therefore we interepret the 'N' relevance label as *not* relevant and the 'R' and 'HR' relevance labels as relevant. A retrieved document that is relevant is called a True Positive (TP) and one that is non-relevant a False Positive (FP). Documents that are not retrieved are not observed in this evaluation so we do not use the True and False Negatives. This is possible for the precision meassure which is defines the fraction of retrieved documents that are relevant.\n",
    "\n",
    "DCG goes beyond binary relevance. The relevance judgments are graded so the emphasis is place on retrieving highly relevant documents. DCG is the total gain accumulated at a particular rank k. The relevance labels for DCG (and also for ERR) are translated to values: 'N' = 0, 'R' = 1, 'HR' = 2. \n",
    "\n",
    "ERR is a cascade based metric. In cascade models the likelihood that a user examines the document at a certain rank, depends\n",
    "on the user's satisfaction of previously observed\n",
    "documents in the ranked list. \n",
    "\n",
    "**Online Evaluations**\n",
    "\n",
    "For the online evaluations we used two interleaving algorithms for the rankings: (1) Team-draft interleaving and (2) Probabilistic Interleaving. For simulating the clicks we used the (1) Random Click Model (RCM) and (2) Position-based Model (PBM). \n",
    "\n",
    "RCM assumes that any document in the ranking can be clicked with the same fixed probability *p*. It uses MLE to estimate *p* by dividing the number of clicks by the number of retrieved documents. In our implementation we calculated the number of retrieved documents by counting all the query actions in the training data and multiply that count by 10, because there are always 10 documents retrieved per query. We made the assumption that a user does not go back to the same query which shows the same 10 documents. The expactation for this model is that E and P bo\n",
    "\n",
    "PBM better because it takes into account the attractiveness and that is based on the true relevance labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_d_meassures = [difference_measures_prec, difference_measures_dcg, difference_measures_err]\n",
    "\n",
    "meassures_per_offline_model = [0,0,0]\n",
    "for i in range(len(all_d_meassures)):\n",
    "    for j in range(len(all_d_meassures[i])):\n",
    "        meassures_per_offline_model[i] += all_d_meassures[i][j]\n",
    "        \n",
    "for i in range(len(meassures_per_offline_model)):\n",
    "    meassures_per_offline_model[i] /= len(all_d_meassures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary1 = plt.figure(num=None, figsize=(7, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "D1 = {u'Precision':meassures_per_offline_model[0], u'DCG': meassures_per_offline_model[1], u'ERR':meassures_per_offline_model[2]}\n",
    "\n",
    "plt.bar(range(len(D1)), D1.values(), align='center')\n",
    "plt.ylabel(\"Delta meassure\")\n",
    "plt.xticks(range(len(D)), D.keys())\n",
    "\n",
    "print '\\n FIGURE 1\\n'\n",
    "print 'Proportions of wins of E:\\n'\n",
    "\n",
    "print 'Team-draft IL & PBM: ', round(meassures_per_model[1],3)\n",
    "print 'Probabilistic IL & PBM: ', round(meassures_per_model[3],3)\n",
    "print 'Team-draft IL & RCM: ', round(meassures_per_model[0],3)\n",
    "print 'Probabilistic IL & RCM: ', round(meassures_per_model[2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meassures_per_online_model = [0,0,0,0]\n",
    "for i in range(len(all_p_meassures)):\n",
    "    for j in range(len(all_p_meassures[i])):\n",
    "        meassures_per_online_model[j] += all_p_meassures[i][j]\n",
    "        \n",
    "for i in range(len(meassures_per_online_model)):\n",
    "    meassures_per_online_model[i] /= len(all_p_meassures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary2 = plt.figure(num=None, figsize=(7, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "D2 = {u'team-draft IL - RCM':meassures_per_model[0], u'team-draft IL - PBM': meassures_per_model[1], u'prob IL - RCM':meassures_per_model[2], u'prob IL - PBM':meassures_per_model[3]}\n",
    "\n",
    "plt.bar(range(len(D)), D.values(), align='center')\n",
    "plt.ylabel(\"Proportion of wins of E\")\n",
    "plt.xticks(range(len(D)), D.keys())\n",
    "\n",
    "print '\\n FIGURE 1\\n'\n",
    "print 'Proportions of wins of E:\\n'\n",
    "\n",
    "print 'Team-draft IL & PBM: ', round(meassures_per_model[1],3)\n",
    "print 'Probabilistic IL & PBM: ', round(meassures_per_model[3],3)\n",
    "print 'Team-draft IL & RCM: ', round(meassures_per_model[0],3)\n",
    "print 'Probabilistic IL & RCM: ', round(meassures_per_model[2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# plots: [[teamRcm:[prec],[dcg],[err]],[teamPBM:[prec],[dcg],[err]],[probRcm:[prec],[dcg],[err]],[probRcm:[prec],[dcg],[err]],\n",
    "plots = [[[[],[]],[[],[]],[[],[]]],[[[],[]],[[],[]],[[],[]]],[[[],[]],[[],[]],[[],[]]],[[[],[]],[[],[]],[[],[]]]]\n",
    "for online_combo in range(len(all_p_meassures[0])):\n",
    "    \n",
    "        for offline_algo in range(len(all_d_meassures)):\n",
    "            for i in range(len(all_p_meassures)): \n",
    "                x = all_p_meassures[i][online_combo]\n",
    "                y = all_d_meassures[offline_algo][i]\n",
    "                plots[online_combo][offline_algo][0].append(x)\n",
    "                plots[online_combo][offline_algo][1].append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '\\nFIGURE 2\\n'\n",
    "\n",
    "f, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8), (ax9, ax10, ax11, ax12)) = plt.subplots(3,4,sharex='col',figsize=(12, 8))\n",
    "\n",
    "# first 4 plots (precisions)\n",
    "ax1.scatter(plots[0][0][0], plots[0][0][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax1.set_title(\"Team-draft IL - RCM (x)\")\n",
    "ax1.set_ylabel(\"Precision (y)\", rotation=0)\n",
    "ax2.scatter(plots[1][0][0], plots[1][0][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax2.set_title(\"Team-draft IL - PBM (x)\")\n",
    "ax3.scatter(plots[2][0][0], plots[2][0][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax3.set_title(\"Probabilisitc IL - RCM (x)\")\n",
    "ax4.scatter(plots[3][0][0], plots[3][0][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax4.set_title(\"Probabilisitc IL - PBM (x)\")\n",
    "\n",
    "# second 4 plots (DCGs)\n",
    "ax5.scatter(plots[0][1][0], plots[0][1][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax5.set_ylabel(\"DCG (y)\", rotation=0)\n",
    "ax6.scatter(plots[1][1][0], plots[1][1][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax7.scatter(plots[2][1][0], plots[2][1][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax8.scatter(plots[3][1][0], plots[3][1][1], edgecolors='none',alpha=0.05,c='red')\n",
    "\n",
    "# last 4 plots (ERRs)\n",
    "ax9.scatter(plots[0][2][0], plots[0][2][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax9.set_ylabel(\"ERR (y)\", rotation=0)\n",
    "ax10.scatter(plots[1][2][0], plots[1][2][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax11.scatter(plots[2][2][0], plots[2][2][1], edgecolors='none',alpha=0.05,c='red')\n",
    "ax12.scatter(plots[3][2][0], plots[3][2][1], edgecolors='none',alpha=0.05,c='red')\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
