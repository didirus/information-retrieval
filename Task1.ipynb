{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval I #\n",
    "## Assignment 2: retrieval models [100 points + 10 bonus points] ##\n",
    "**TA**: Christophe Van Gysel (cvangysel@uva.nl; C3.258B, Science Park 904)\n",
    "\n",
    "**Secondary TAs**: Harrie Oosterhuis, Nikos Voskarides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "from itertools import islice\n",
    "import subprocess\n",
    "import random\n",
    "import re\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = pyndri.Index('index/')\n",
    "token2id, id2token, _ = index.get_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = index.maximum_document()-index.document_base()\n",
    "\n",
    "def get_rid_of_zeros(n):\n",
    "    collection = []\n",
    "    for i in range(1,n+1):\n",
    "        collection.append([word for word in index.document(i)[1] if word > 0])\n",
    "    return collection\n",
    "\n",
    "collection = get_rid_of_zeros(n)\n",
    "# pickle.dump(collection, open(\"./results/collection.p\", \"wb\"))\n",
    "# collection = pickle.load(open( \"./results/collection.p\", \"rb\"))\n",
    "\n",
    "def get_collection_length():\n",
    "    length_collection = 0\n",
    "    for i in range(n):\n",
    "        length_collection += len(collection[i])\n",
    "    return length_collection\n",
    "\n",
    "col_len = get_collection_length()\n",
    "\n",
    "def get_unique_collection(n):\n",
    "    unique_words_docs = []\n",
    "    for doc in collection:\n",
    "        unique_words_docs.append(list(set(doc)))\n",
    "    return unique_words_docs\n",
    "\n",
    "unique_words_docs = get_unique_collection(n)\n",
    "# pickle.dump(unique_words_docs, open(\"./results/unique_words_docs.p\", \"wb\"))\n",
    "# unique_words_docs = pickle.load(open( \"./results/unique_words_docs.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Inverted Index List **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_inverted_index():\n",
    "    inverted_list = {}\n",
    "    query_list = []\n",
    "\n",
    "    with open('./ap_88_89/topics_title', 'r') as f_topics:   \n",
    "        for query in parse_topics([f_topics]).items():\n",
    "            query_list.append(query)\n",
    "\n",
    "    nr = 0\n",
    "    for query in query_list:\n",
    "        nr += 1\n",
    "        if nr % 10 == 0: #\n",
    "            print('\\r',str(nr)+'/'+str(len(query_list)))\n",
    "\n",
    "        # getting the query term ids\n",
    "        query_id = query[0]\n",
    "        query_string = query[1]\n",
    "        query_tokens = index.tokenize(query_string)\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "\n",
    "        for qti in query_id_tokens: # for every query term\n",
    "            if qti not in inverted_list: # only check unique query token once\n",
    "                tot_count = 0\n",
    "                inverted_list[qti] = [[],0]\n",
    "                for i in range(n):                \n",
    "                    word_counter = collection[i].count(qti)\n",
    "                    if word_counter > 0:\n",
    "                        tot_count += word_counter\n",
    "                        docid = index.document_ids([index.document(i+1)[0]])[0][1]\n",
    "                        inverted_list[qti][0].append(docid) # add document to query tok id\n",
    "                inverted_list[qti][1]= (tot_count)\n",
    "    return inverted_list, query_list\n",
    "\n",
    "inverted_list, query_list = get_inverted_index()\n",
    "# pickle.dump(inverted_list, open(\"./results/inverted_index.p\", \"wb\"))\n",
    "# pickle.dump(query_list, open(\"./results/query_list.p\", \"wb\"))\n",
    "# query_list = pickle.load(open( \"./results/query_list.p\", \"rb\"))\n",
    "# inverted_list = pickle.load(open( \"./results/inverted_index.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More setups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the query ids of the validation list\n",
    "with open('./ap_88_89/qrel_validation', 'r') as val_queries: \n",
    "    val_queries = list(set([line.split(' ')[0] for line in val_queries]))\n",
    "\n",
    "    \n",
    "# the query ids of the test list\n",
    "with open('./ap_88_89/qrel_test', 'r') as test_queries_: \n",
    "    test_queries = list(set([line.split(' ')[0] for line in test_queries_]))\n",
    "    \n",
    "queries_dict = {} # {qid: qstring, qid: qstring...}\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics: \n",
    "    for query in parse_topics([f_topics]).items():\n",
    "        queries_dict[query[0]] = query[1]\n",
    "        \n",
    "def make_dict_format(dic):\n",
    "    results = []\n",
    "    for doc, score in dic.items():\n",
    "        results.append((score, doc))\n",
    "    return tuple(results)\n",
    "\n",
    "def get_query_docs(queryset):\n",
    "    query_docus = {}\n",
    "    nr = 0\n",
    "    for query_id in queryset:\n",
    "        nr +=1\n",
    "        if nr % 5 ==0:\n",
    "            print('doc', nr)\n",
    "        query_tokens = index.tokenize(queries_dict[str(query_id)])\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "\n",
    "        relev_documents = []\n",
    "        for token in query_id_tokens:\n",
    "            for doc in inverted_list[token][0]:\n",
    "                if doc not in relev_documents:\n",
    "                    relev_documents.append(doc)\n",
    "\n",
    "        query_docus[query_id] = relev_documents\n",
    "    return query_docus\n",
    "\n",
    "query_val_docs = get_query_docs(val_queries)\n",
    "# pickle.dump(query_val_docs, open(\"./results/query_val_docs.p\", \"wb\"))\n",
    "\n",
    "query_test_docs = get_query_docs(test_queries)\n",
    "# pickle.dump(query_test_docs, open(\"./results/query_test_docs.p\", \"wb\"))\n",
    "\n",
    "# query_val_docs = pickle.load(open( \"./results/query_val_docs.p\", \"rb\"))\n",
    "# query_test_docs = pickle.load(open( \"./results/query_test_docs.p\", \"rb\"))\n",
    "\n",
    "def idf(t):\n",
    "    return math.log(n)-math.log(len(inverted_list[t][0]))\n",
    "\n",
    "def background_prob(w):\n",
    "    if w in inverted_list:\n",
    "        tf_w_C = inverted_list[w][-1]\n",
    "    else: \n",
    "        tf_w_C = 0\n",
    "    return tf_w_C/float(col_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF  (Vector-space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_idf(t, d):\n",
    "    return math.log(1+collection[d].count(t)) * idf(t)\n",
    "\n",
    "def score_TFIDF(q,d):\n",
    "    unique = unique_words_docs[d-1]\n",
    "    score = 0\n",
    "    for word in q:\n",
    "        score += tf_idf(word,d-1)   \n",
    "    return score\n",
    "\n",
    "def get_TFIDF_scores():\n",
    "    TFIDF_dict = {}\n",
    "    \n",
    "    nr = 0\n",
    "    for query_id in test_queries:\n",
    "        r = {}\n",
    "        print('\\r',str(nr)+'/'+str(len(test_queries)), end=\" \")\n",
    "        query_tokens = index.tokenize(queries_dict[str(query_id)])\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "\n",
    "        for d in query_test_docs[query_id]:\n",
    "            d_id = str(index.document(d)[0])\n",
    "            r[d_id] = score_TFIDF(query_id_tokens, d)\n",
    "            \n",
    "        while len(r) < 1000:\n",
    "            i = random.randrange(1,n)\n",
    "            i = str(index.document(i)[0])\n",
    "            if len(collection[i-1]) == 0:\n",
    "                r[j] = r[j]\n",
    "            else: r[i] = 0 if i not in r else r[i]\n",
    "\n",
    "        TFIDF_dict[str(query_id)] = make_dict_format(r)\n",
    "        nr += 1\n",
    "        \n",
    "    return TFIDF_dict\n",
    "\n",
    "TFIDF_scores = get_TFIDF_scores()\n",
    "# pickle.dump(TFIDF_scores, open(\"./results/tfidf_results.p\", \"wb\"))\n",
    "# TFIDF_scores = pickle.load(open( \"./results/tfidf_results.p\", \"rb\"))\n",
    "\n",
    "def get_top_tfidf():\n",
    "    tfidf_top_1000_docs = {}\n",
    "    for key, values in TFIDF_scores.items():\n",
    "        query_id = key\n",
    "        tfidf_top_1000_docs[int(query_id)] = []\n",
    "        sorted_list = sorted(values, key=itemgetter(0), reverse = True)[:1000]\n",
    "        for value in sorted_list:\n",
    "            tfidf_top_1000_docs[int(query_id)].append(index.document_ids([value[1]])[0][1])\n",
    "    return tfidf_top_1000_docs\n",
    "\n",
    "tfidf_top_1000_docs = get_top_tfidf()\n",
    "# pickle.dump(tfidf_top_1000_docs, open(\"./results/tfidf_top.p\", \"wb\"))\n",
    "# tfidf_top_1000_docs = pickle.load(open( \"./results/tfidf_top.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 (Probabilistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_length():\n",
    "    l = 0\n",
    "    for i in range(n):\n",
    "        l += len(collection[i])\n",
    "    return l/float(n)\n",
    "\n",
    "l_av = average_length()\n",
    "\n",
    "def BM25(t,d):\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    first = ((k1+1)* collection[d].count(t)) / (k1*((1-b)+b*(len(collection[d])/l_av))+collection[d].count(t))\n",
    "    return  first*idf(t)\n",
    "\n",
    "def score_BM25(q,d):\n",
    "    unique = unique_words_docs[d-1]\n",
    "    score = 0\n",
    "    for word in set(q):\n",
    "        score += BM25(word,d-1)\n",
    "        \n",
    "    return score\n",
    "\n",
    "def get_BM25_scores():\n",
    "    BM25_dict = {}\n",
    "    \n",
    "    nr = 0\n",
    "    for query_id in test_queries:\n",
    "        r = {}\n",
    "        print('\\r',str(nr)+'/'+str(len(test_queries)), end=\" \")\n",
    "        query_tokens = index.tokenize(queries_dict[str(query_id)])\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "        for d in query_test_docs[query_id]:\n",
    "            d_id = str(index.document(d)[0])\n",
    "            r[d_id] = score_BM25(query_id_tokens, d)\n",
    "            \n",
    "        while len(r) < 1000:\n",
    "            i = random.randrange(1,n)\n",
    "            i = str(index.document(i)[0])\n",
    "            if len(collection[i-1]) == 0:\n",
    "                r[j] = r[j]\n",
    "            else: r[i] = 0 if i not in r else r[i]\n",
    "\n",
    "        BM25_dict[str(query_id)] = make_dict_format(r)\n",
    "        nr += 1\n",
    "        \n",
    "    return BM25_dict\n",
    "\n",
    "BM25_scores = get_BM25_scores()\n",
    "# pickle.dump(BM25_scores, open(\"./results/bm25_results.p\", \"wb\"))\n",
    "# BM25_scores = pickle.load(open( \"./results/bm25_results.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jelinek-Mercer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jelinek_mercer(lamb, d, w):\n",
    "    P = (lamb * (collection[d].count(w)/len(collection[d]))) + ((1-lamb) * background_prob(w))\n",
    "    return P\n",
    "\n",
    "def jelinek_score(lamb,query,d):\n",
    "    score = 0\n",
    "    for q in query:\n",
    "        score += math.log(jelinek_mercer(lamb,d,q))\n",
    "    return score\n",
    "\n",
    "def get_jelinek_scores(lamb, type_set):\n",
    "    print('\\r','lamb:',lamb)\n",
    "    jelinek_dict = {}\n",
    "    nr = 0\n",
    "    \n",
    "    if type_set == \"val\":\n",
    "        queries = val_queries\n",
    "        query_docs = query_val_docs\n",
    "    else:\n",
    "        queries = test_queries\n",
    "        query_docs = query_test_docs\n",
    "        \n",
    "    for query_id in queries:\n",
    "        r = {}\n",
    "        print('\\r',str(nr)+'/'+str(len(queries)), end=\" \")\n",
    "        query_tokens = index.tokenize(queries_dict[str(query_id)])\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "        \n",
    "    \n",
    "        for d in query_docs[query_id]:\n",
    "            d_id = str(index.document(d)[0])\n",
    "            r[d_id] = jelinek_score(lamb, query_id_tokens, d-1)\n",
    "            \n",
    "        while len(r) < 1000:\n",
    "            i = random.randrange(1,n)\n",
    "            j = str(index.document(i)[0])\n",
    "            if len(collection[i-1]) == 0:\n",
    "                r[j] = r[j]\n",
    "            else: r[j] = jelinek_score(lamb, query_id_tokens, i-1) if j not in r else r[j]\n",
    "\n",
    "        jelinek_dict[str(query_id)] = make_dict_format(r)\n",
    "        nr += 1\n",
    "        \n",
    "    return jelinek_dict\n",
    "\n",
    "jelinek_scores_0_1 = get_jelinek_scores(0.1, 'val')\n",
    "jelinek_scores_0_2 = get_jelinek_scores(0.2, 'val')\n",
    "jelinek_scores_0_3 = get_jelinek_scores(0.3, 'val')\n",
    "jelinek_scores_0_4 = get_jelinek_scores(0.4, 'val')\n",
    "jelinek_scores_0_5 = get_jelinek_scores(0.5, 'val')\n",
    "jelinek_scores_0_6 = get_jelinek_scores(0.6, 'val')\n",
    "jelinek_scores_0_7 = get_jelinek_scores(0.7, 'val')\n",
    "jelinek_scores_0_8 = get_jelinek_scores(0.8, 'val')\n",
    "jelinek_scores_0_9 = get_jelinek_scores(0.9, 'val')\n",
    "# pickle.dump(jelinek_scores_0_1, open(\"./results/jelinek_scores_0_1.p\", \"wb\"))\n",
    "# pickle.dump(jelinek_scores_0_2, open(\"./results/jelinek_scores_0_2.p\", \"wb\"))\n",
    "# pickle.dump(jelinek_scores_0_3, open(\"./results/jelinek_scores_0_3.p\", \"wb\"))\n",
    "# pickle.dump(jelinek_scores_0_4, open(\"./results/jelinek_scores_0_4.p\", \"wb\"))\n",
    "# pickle.dump(jelinek_scores_0_5, open(\"./results/jelinek_scores_0_5.p\", \"wb\"))\n",
    "# pickle.dump(jelinek_scores_0_6, open(\"./results/jelinek_scores_0_6.p\", \"wb\"))\n",
    "# pickle.dump(jelinek_scores_0_7, open(\"./results/jelinek_scores_0_7.p\", \"wb\"))\n",
    "# pickle.dump(jelinek_scores_0_8, open(\"./results/jelinek_scores_0_8.p\", \"wb\"))\n",
    "# pickle.dump(jelinek_scores_0_9, open(\"./results/jelinek_scores_0_9.p\", \"wb\"))\n",
    "# jelinek_scores_0_1 = pickle.load(open( \"./results/jelinek_scores_0_1.p\", \"rb\"))\n",
    "# jelinek_scores_0_2 = pickle.load(open( \"./results/jelinek_scores_0_2.p\", \"rb\"))\n",
    "# jelinek_scores_0_3 = pickle.load(open( \"./results/jelinek_scores_0_3.p\", \"rb\"))\n",
    "# jelinek_scores_0_4 = pickle.load(open( \"./results/jelinek_scores_0_4.p\", \"rb\"))\n",
    "# jelinek_scores_0_5 = pickle.load(open( \"./results/jelinek_scores_0_5.p\", \"rb\"))\n",
    "# jelinek_scores_0_6 = pickle.load(open( \"./results/jelinek_scores_0_6.p\", \"rb\"))\n",
    "# jelinek_scores_0_7 = pickle.load(open( \"./results/jelinek_scores_0_7.p\", \"rb\"))\n",
    "# jelinek_scores_0_8 = pickle.load(open( \"./results/jelinek_scores_0_8.p\", \"rb\"))\n",
    "# jelinek_scores_0_9 = pickle.load(open( \"./results/jelinek_scores_0_9.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dirichlet_prior(mu,d,w):\n",
    "    return ((len(collection[d])/(len(collection[d])+mu)) * (collection[d].count(w)/len(collection[d]))) + ((mu/(mu+len(collection[d]))) * background_prob(w))\n",
    "\n",
    "def dirichlet_score(mu,query,d):\n",
    "    score = 0\n",
    "    for q in query:\n",
    "        score += math.log(dirichlet_prior(mu,d,q))\n",
    "    return score\n",
    "\n",
    "def get_dirichlet_scores(mu, type_set):\n",
    "    print('\\r','mu:',mu)\n",
    "    dirichlet_dict = {}\n",
    "    nr = 0\n",
    "    \n",
    "    if type_set == \"val\":\n",
    "        queries = val_queries\n",
    "        query_docs = query_val_docs\n",
    "    else:\n",
    "        queries = test_queries\n",
    "        query_docs = query_test_docs\n",
    "        \n",
    "    for query_id in queries:\n",
    "        r = {}\n",
    "        print('\\r',str(nr)+'/'+str(len(queries)), end=\" \")\n",
    "        query_tokens = index.tokenize(queries_dict[str(query_id)])\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]      \n",
    "    \n",
    "        for d in query_docs[query_id]:\n",
    "            d_id = str(index.document(d)[0])\n",
    "            r[d_id] = dirichlet_score(mu, query_id_tokens, d-1)\n",
    "            \n",
    "        while len(r) < 1000:\n",
    "            i = random.randrange(1,n)\n",
    "            j = str(index.document(i)[0])\n",
    "            if len(collection[i-1]) == 0:\n",
    "                r[j] = r[j]\n",
    "            else: r[j] = dirichlet_score(mu, query_id_tokens, i-1) if j not in r else r[j]\n",
    "\n",
    "        dirichlet_dict[str(query_id)] = make_dict_format(r)\n",
    "        nr += 1\n",
    "        \n",
    "    return dirichlet_dict\n",
    "\n",
    "dirichlet_scores_500 = get_dirichlet_scores(500, \"val\")\n",
    "dirichlet_scores_1000 = get_dirichlet_scores(1000, \"val\")\n",
    "dirichlet_scores_1500 = get_dirichlet_scores(1500, \"val\")\n",
    "dirichlet_scores_2000 = get_dirichlet_scores(2000, \"val\")\n",
    "# pickle.dump(dirichlet_scores_500, open(\"./results/dirichlet_scores_500.p\", \"wb\"))\n",
    "# pickle.dump(dirichlet_scores_1000, open(\"./results/dirichlet_scores_1000.p\", \"wb\"))\n",
    "# pickle.dump(dirichlet_scores_1500, open(\"./results/dirichlet_scores_1500.p\", \"wb\"))\n",
    "# pickle.dump(dirichlet_scores_2000, open(\"./results/dirichlet_scores_2000.p\", \"wb\"))\n",
    "# dirichlet_scores_500 = pickle.load(open( \"./results/dirichlet_scores_500.p\", \"rb\"))\n",
    "# dirichlet_scores_1000 = pickle.load(open( \"./results/dirichlet_scores_1000.p\", \"rb\"))\n",
    "# dirichlet_scores_1500 = pickle.load(open( \"./results/dirichlet_scores_1500.p\", \"rb\"))\n",
    "# dirichlet_scores_2000 = pickle.load(open( \"./results/dirichlet_scores_2000.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Discounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def absolute_discounting(delta, d, w):\n",
    "    return (max(collection[d].count(w)-delta, 0)/len(collection[d])) + (((delta * len(unique_words_docs[d]))/len(collection[d])) * (background_prob(w)))\n",
    "\n",
    "def AD_score(delta,query,d):\n",
    "    score = 0\n",
    "    for q in query:\n",
    "        score += math.log(absolute_discounting(delta,d,q))\n",
    "    return score\n",
    "\n",
    "def get_AD_scores(delta, type_set):\n",
    "    print('\\r','delta:',delta)\n",
    "    AD_dict = {}\n",
    "    nr = 1 \n",
    "    \n",
    "    if type_set == \"val\":\n",
    "        queries = val_queries\n",
    "        query_docs = query_val_docs\n",
    "    else:\n",
    "        queries = test_queries\n",
    "        query_docs = query_test_docs\n",
    "        \n",
    "    for query_id in queries:\n",
    "        r = {}\n",
    "        print('\\r',str(nr)+'/'+str(len(queries)), end=\" \")\n",
    "        query_tokens = index.tokenize(queries_dict[str(query_id)])\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]      \n",
    "    \n",
    "        for d in query_docs[query_id]:\n",
    "            d_id = str(index.document(d)[0])\n",
    "            r[d_id] = AD_score(delta, query_id_tokens, d-1)\n",
    "            \n",
    "        while len(r) < 1000:\n",
    "            i = random.randrange(1,n)\n",
    "            j = str(index.document(i)[0])\n",
    "            if len(collection[i-1]) == 0:\n",
    "                r[j] = r[j]\n",
    "            else: r[j] = AD_score(delta, query_id_tokens, i-1) if j not in r else r[j]\n",
    "\n",
    "        AD_dict[str(query_id)] = make_dict_format(r)\n",
    "        nr += 1\n",
    "        \n",
    "    return AD_dict\n",
    "\n",
    "AD_scores_0_1 = get_AD_scores(0.1,\"val\")\n",
    "AD_scores_0_2 = get_AD_scores(0.2,\"val\")\n",
    "AD_scores_0_3 = get_AD_scores(0.3,\"val\")\n",
    "AD_scores_0_4 = get_AD_scores(0.4,\"val\")\n",
    "AD_scores_0_5 = get_AD_scores(0.5,\"val\")\n",
    "AD_scores_0_6 = get_AD_scores(0.6,\"val\")\n",
    "AD_scores_0_7 = get_AD_scores(0.7,\"val\")\n",
    "AD_scores_0_8 = get_AD_scores(0.8,\"val\")\n",
    "AD_scores_0_9 = get_AD_scores(0.9,\"val\")\n",
    "# pickle.dump(AD_scores_0_1, open(\"./results/AD_scores_0_1.p\", \"wb\"))\n",
    "# pickle.dump(AD_scores_0_2, open(\"./results/AD_scores_0_2.p\", \"wb\"))\n",
    "# pickle.dump(AD_scores_0_3, open(\"./results/AD_scores_0_3.p\", \"wb\"))\n",
    "# pickle.dump(AD_scores_0_4, open(\"./results/AD_scores_0_4.p\", \"wb\"))\n",
    "# pickle.dump(AD_scores_0_5, open(\"./results/AD_scores_0_5.p\", \"wb\"))\n",
    "# pickle.dump(AD_scores_0_6, open(\"./results/AD_scores_0_6.p\", \"wb\"))\n",
    "# pickle.dump(AD_scores_0_7, open(\"./results/AD_scores_0_7.p\", \"wb\"))\n",
    "# pickle.dump(AD_scores_0_8, open(\"./results/AD_scores_0_8.p\", \"wb\"))\n",
    "# pickle.dump(AD_scores_0_9, open(\"./results/AD_scores_0_9.p\", \"wb\"))\n",
    "# AD_scores_0_1 = pickle.load(open( \"./results/AD_scores_0_1.p\", \"rb\"))\n",
    "# AD_scores_0_2 = pickle.load(open( \"./results/AD_scores_0_2.p\", \"rb\"))\n",
    "# AD_scores_0_3 = pickle.load(open( \"./results/AD_scores_0_3.p\", \"rb\"))\n",
    "# AD_scores_0_4 = pickle.load(open( \"./results/AD_scores_0_4.p\", \"rb\"))\n",
    "# AD_scores_0_5 = pickle.load(open( \"./results/AD_scores_0_5.p\", \"rb\"))\n",
    "# AD_scores_0_6 = pickle.load(open( \"./results/AD_scores_0_6.p\", \"rb\"))\n",
    "# AD_scores_0_7 = pickle.load(open( \"./results/AD_scores_0_7.p\", \"rb\"))\n",
    "# AD_scores_0_8 = pickle.load(open( \"./results/AD_scores_0_8.p\", \"rb\"))\n",
    "# AD_scores_0_9 = pickle.load(open( \"./results/AD_scores_0_9.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write run, create output and analyse output functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "# write_run(\n",
    "#     model_name=\"PLM\",\n",
    "#     data=PLM_scores,\n",
    "#     out_f=open(\"results/PLM_scores.run\", \"w\"),\n",
    "#     max_objects_per_query=1000)\n",
    "\n",
    "r = re.compile(r'([^ \\\\t]*)\\\\t*')\n",
    "\n",
    "def create_output(type_set, filename):\n",
    "    if type_set == 'test':\n",
    "        command = \"trec_eval -m all_trec -q ap_88_89/qrel_test \"\n",
    "    else:\n",
    "        command = \"trec_eval -m all_trec -q ap_88_89/qrel_validation \"\n",
    "    command +=  \"runfiles/\" + filename #+\" | grep -E '\\sall\\s'\"\n",
    "    \n",
    "    output = str(subprocess.check_output(command, shell = True))\n",
    "    return output\n",
    "\n",
    "def analyse_output(output, title):\n",
    "    # NDCG@10, Mean Average Precision (MAP@1000), Precision@5 and Recall@1000.\n",
    "    measure_results = {}\n",
    "    measures = [\"ndcg_cut_10\", [\"100\"]], [\"map_cut_1000\",[]], [\"P_5\", [\"500\", \"relative\"]], [\"recall_1000\",[]]\n",
    "    for measure in measures:\n",
    "        measure_list = []\n",
    "        measure_all = 0\n",
    "        for line in output.split():\n",
    "            if measure[0] in line:\n",
    "                clean = True\n",
    "                for restriction in measure[1]:\n",
    "                    if restriction in line:\n",
    "                        clean = False\n",
    "                if clean:\n",
    "                    if \"tall\" in line:\n",
    "                        measure_all = r.findall(line)[-1]\n",
    "                    else:\n",
    "                        measure_list.append(float(r.findall(line)[-1]))\n",
    "        measure_results[measure[0]] = measure_all, measure_list\n",
    "\n",
    "    return [title, measure_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet mu optimisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output500 = create_output('validation', 'dirichlet_scores_500.run')\n",
    "measure_results500 = analyse_output(output500, \"Dirichlet mu=500\")\n",
    "output1000 = create_output('validation', 'dirichlet_scores_1000.run')\n",
    "measure_results1000 = analyse_output(output1000, \"Dirichlet mu=1000\")\n",
    "output1500 = create_output('validation', 'dirichlet_scores_1500.run')\n",
    "measure_results1500 = analyse_output(output1500, \"Dirichlet mu=1500\")\n",
    "output2000 = create_output('validation', 'dirichlet_scores_2000.run')\n",
    "measure_results2000 = analyse_output(output2000, \"Dirichlet mu=2000\")\n",
    "dirichlet_measures = [measure_results500, measure_results1000, measure_results1500, measure_results2000]\n",
    "\n",
    "for param in dirichlet_measures:\n",
    "    print(param[0])\n",
    "    for key,value in param[1].items():\n",
    "        print(str(key)+':', value[0])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** -> Dirichlet mu NDCG winner: 1000**\n",
    "\n",
    "\n",
    "** -> Dirichlet overall winner: 2000**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jelinek lambda optimisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outl01 = create_output('validation', 'jelinek_scores_0_1.run')\n",
    "outl02 = create_output('validation', 'jelinek_scores_0_2.run')\n",
    "outl03 = create_output('validation', 'jelinek_scores_0_3.run')\n",
    "outl04 = create_output('validation', 'jelinek_scores_0_4.run')\n",
    "outl05 = create_output('validation', 'jelinek_scores_0_5.run')\n",
    "outl06 = create_output('validation', 'jelinek_scores_0_6.run')\n",
    "outl07 = create_output('validation', 'jelinek_scores_0_7.run')\n",
    "outl08 = create_output('validation', 'jelinek_scores_0_8.run')\n",
    "outl09 = create_output('validation', 'jelinek_scores_0_9.run')\n",
    "resl01 = analyse_output(outl01, \"Jelinek lamb=0.1\")\n",
    "resl02 = analyse_output(outl02, \"Jelinek lamb=0.2\")\n",
    "resl03 = analyse_output(outl03, \"Jelinek lamb=0.3\")\n",
    "resl04 = analyse_output(outl04, \"Jelinek lamb=0.4\")\n",
    "resl05 = analyse_output(outl05, \"Jelinek lamb=0.5\")\n",
    "resl06 = analyse_output(outl06, \"Jelinek lamb=0.6\")\n",
    "resl07 = analyse_output(outl07, \"Jelinek lamb=0.7\")\n",
    "resl08 = analyse_output(outl08, \"Jelinek lamb=0.8\")\n",
    "resl09 = analyse_output(outl09, \"Jelinek lamb=0.9\")\n",
    "jelinek_measures = [resl01, resl02, resl03, resl04, resl05, resl06, resl07, resl08, resl09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for param in jelinek_measures:\n",
    "    print(param[0])\n",
    "    for key,value in param[1].items():\n",
    "        print(str(key)+':', value[0])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** -> Jelinek lambda winner: 0.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Discounting delta optimisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outd01 = create_output('validation', 'AD_scores_0_1.run')\n",
    "outd02 = create_output('validation', 'AD_scores_0_2.run')\n",
    "outd03 = create_output('validation', 'AD_scores_0_3.run')\n",
    "outd04 = create_output('validation', 'AD_scores_0_4.run')\n",
    "outd05 = create_output('validation', 'AD_scores_0_5.run')\n",
    "outd06 = create_output('validation', 'AD_scores_0_6.run')\n",
    "outd07 = create_output('validation', 'AD_scores_0_7.run')\n",
    "outd08 = create_output('validation', 'AD_scores_0_8.run')\n",
    "outd09 = create_output('validation', 'AD_scores_0_9.run')\n",
    "resd01 = analyse_output(outd01, \"AD delta=0.1\")\n",
    "resd02 = analyse_output(outd02, \"AD delta=0.2\")\n",
    "resd03 = analyse_output(outd03, \"AD delta=0.3\")\n",
    "resd04 = analyse_output(outd04, \"AD delta=0.4\")\n",
    "resd05 = analyse_output(outd05, \"AD delta=0.5\")\n",
    "resd06 = analyse_output(outd06, \"AD delta=0.6\")\n",
    "resd07 = analyse_output(outd07, \"AD delta=0.7\")\n",
    "resd08 = analyse_output(outd08, \"AD delta=0.8\")\n",
    "resd09 = analyse_output(outl09, \"AD delta=0.9\")\n",
    "AD_measures = [resd01, resd02, resd03, resd04, resd05, resd06, resd07, resd08, resd09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for param in AD_measures:\n",
    "    print(param[0])\n",
    "    for key,value in param[1].items():\n",
    "        print(str(key)+':', value[0])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** -> AD delta winner: 0.8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now run the winning parameters on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Jelinek\n",
    "jelinek_scores_test = get_jelinek_scores(0.2, 'test')\n",
    "# pickle.dump(jelinek_scores_test, open(\"./results/jelinek_scores_test.p\", \"wb\"))\n",
    "# jelinek_scores_test = pickle.load(open( \"./results/jelinek_scores_test.p\", \"rb\"))\n",
    "outl02t = create_output('test', 'jelinek_scores_test.run')\n",
    "resl02t = analyse_output(outl02t, \"Jelinek lamb=0.2\")\n",
    "print(resl02t[0])\n",
    "for key,value in resl02t[1].items():\n",
    "    print(str(key)+':', value[0])\n",
    "print('')\n",
    "\n",
    "#Dirichlet\n",
    "dirichlet_scores_test = get_dirichlet_scores(2000, \"test\")\n",
    "# pickle.dump(dirichlet_scores_test, open(\"./results/dirichlet_scores_test.p\", \"wb\"))\n",
    "# dirichlet_scores_test = pickle.load(open( \"./results/dirichlet_scores_test.p\", \"rb\"))\n",
    "output2000t = create_output('test', 'dirichlet_scores_test.run')\n",
    "measure_results2000t = analyse_output(output2000t, \"Dirichlet mu=2000\")\n",
    "print(measure_results2000t[0])\n",
    "for key,value in measure_results2000t[1].items():\n",
    "    print(str(key)+':', value[0])\n",
    "print('')\n",
    "\n",
    "#Absolute Discounting\n",
    "AD_scores_test = get_AD_scores(0.8,\"test\")\n",
    "# pickle.dump(AD_scores_test, open(\"./results/AD_scores_test.p\", \"wb\"))\n",
    "# AD_scores_test = pickle.load(open( \"./results/AD_scores_test.p\", \"rb\"))\n",
    "outd08t = create_output('test', 'AD_scores_test.run')\n",
    "resd08t = analyse_output(outd08t, \"AD delta=0.8\")\n",
    "print(resd08t[0])\n",
    "for key,value in resd08t[1].items():\n",
    "    print(str(key)+':', value[0])\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Language Models on Test Set with mu for Dirichlet = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_query_terms():\n",
    "    query_terms =[]\n",
    "    for query in query_list:\n",
    "        query_tokens = index.tokenize(query[1])\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "\n",
    "        for query_token in query_id_tokens:\n",
    "            if query_token not in query_terms:\n",
    "                query_terms.append(query_token)\n",
    "    return query_terms\n",
    "\n",
    "get_query_terms()\n",
    "\n",
    "def q_docs():\n",
    "    q_docs = []\n",
    "    nr = 0\n",
    "    for doc in collection:\n",
    "        nr +=1\n",
    "        if nr % 1000 ==0:\n",
    "            print(nr)\n",
    "        query_doc = []\n",
    "        for i in range(len(doc)):\n",
    "            if doc[i] in query_terms:\n",
    "                for qterm in query_terms:\n",
    "                    if qterm == doc[i]:\n",
    "                        query_doc.append([qterm, i])\n",
    "        q_docs.append(query_doc)\n",
    "    return q_docs\n",
    "\n",
    "q_docs = get_q_docs()\n",
    "# pickle.dump(q_docs, open(\"./results/query_docs.p\", \"wb\"))\n",
    "# q_docs = pickle.load(open( \"./results/query_docs.p\", \"rb\"))\n",
    "\n",
    "def kernel_gaussian(sigma,i,j):\n",
    "    return math.exp((-1*((i-j)**2))/(2*(sigma**2)))\n",
    "    \n",
    "def kernel_triangle(sigma,i,j):\n",
    "    if i-j <= sigma:\n",
    "        return 1-((i-j)/sigma)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def kernel_cosine(sigma,i,j):\n",
    "    if i-j <= sigma:\n",
    "        return 0.5*(1+math.cos(((i-j)*math.pi)/sigma))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def kernel_circle(sigma,i,j):\n",
    "    if i-j <= sigma:\n",
    "        return math.sqrt(1-(((i-j)/sigma)**2))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def kernel_passage(sigma,i,j):\n",
    "    if i-j <= sigma:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0  \n",
    "    \n",
    "def c(w,j,d): \n",
    "    if w == d[j]:\n",
    "        return 1\n",
    "    else: return 0\n",
    "    \n",
    "def c_prime(w,i,d): # 0.0003 seconds\n",
    "    c_prime = 0\n",
    "    \n",
    "    for query in q_docs[d]:\n",
    "        if query[0] == w:\n",
    "            j = query[1]\n",
    "            c_prime += kernel_gaussian(50,i,j)\n",
    "    return c_prime\n",
    "\n",
    "def get_all_zs():\n",
    "    Z = []\n",
    "    \n",
    "    max_len = 0\n",
    "    for doc in collection:\n",
    "        if len(doc) > max_len:\n",
    "            max_len = len(doc)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        z = 0\n",
    "        for j in range(max_len):\n",
    "            z += kernel_gaussian(50,i,j)\n",
    "        Z.append(z)\n",
    "    return Z\n",
    "\n",
    "Zs = get_all_zs()\n",
    "\n",
    "def PLM(mu,w,i,d): \n",
    "    P = (c_prime(w,i,d) + (mu * background_prob(w))) / (Zs[i] + mu)#\n",
    "    return P\n",
    "\n",
    "def PLM_score(q,d):\n",
    "    unique = unique_words_docs[d]\n",
    "    max_score = -100000\n",
    "    for i in range(len(collection[d])):   \n",
    "        scores = []\n",
    "        for word in q:\n",
    "            if inverted_list[word][0]:\n",
    "                scores.append(((q.count(word)/float(len(q))) * (math.log((q.count(word)/float(len(q)))/PLM(1000,word,i,d)))))\n",
    "        if -sum(scores) > max_score:\n",
    "            max_score = -sum(scores)\n",
    "    return max_score\n",
    "\n",
    "def get_PLM_scores(mu, type_set):\n",
    "    PLM_dict = {}\n",
    "    secdoc = 0\n",
    "    nr = 0\n",
    "    \n",
    "    if type_set == \"val\":\n",
    "        queries = val_queries\n",
    "        query_docs = query_val_docs\n",
    "    else:\n",
    "        queries = test_queries\n",
    "        query_docs = query_test_docs\n",
    "        \n",
    "    for query_id in queries:\n",
    "        nr +=1\n",
    "        start = time.time()\n",
    "        r = {}\n",
    "        if len(query_docs[query_id]) < 1000:\n",
    "            nrdocs = 1000\n",
    "        else: nrdocs = len(query_docs[query_id])\n",
    "        print('\\r','Q'+str(query_id), 'estimated time: '+ str(round((secdoc*nrdocs)/60.0,2)), 'min,', nrdocs, 'documents', '\\t'+str(nr)+'/'+str(len(queries)), end=\" \") \n",
    "        query_tokens = index.tokenize(queries_dict[str(query_id)]) #query[1])\n",
    "        query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "        query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "\n",
    "        for d in query_docs[query_id]:\n",
    "            d_id = str(index.document(d)[0])\n",
    "            r[d_id] = PLM_score(query_id_tokens, d-1)\n",
    "\n",
    "            \n",
    "        while len(r) < 1000:\n",
    "            i = random.randrange(1,n)\n",
    "            j = str(index.document(i)[0])\n",
    "            if len(collection[i-1]) == 0:\n",
    "                r[j] = r[j]\n",
    "            else: r[j] = PLM_score(query_id_tokens, i-1) if j not in r else r[j]\n",
    "                \n",
    "        PLM_dict[str(query_id)] = make_dict_format(r)\n",
    "        secdoc = (time.time()-start)/float(nrdocs)\n",
    "    \n",
    "    return PLM_dict\n",
    "\n",
    "PLM_scores = get_PLM_scores(2000, 'test')\n",
    "# pickle.dump(PLM_scores, open(\"./results/plm_results.p\", \"wb\"))\n",
    "# PLM_scores = pickle.load(open( \"./results/PLM_scores.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
