{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trec Eval and Significance Testing of all 10 Methods of Task 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write run, get output and analyse output functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "# write_run(\n",
    "#     model_name=\"PLM\",\n",
    "#     data=PLM_scores,\n",
    "#     out_f=open(\"results/PLM_scores.run\", \"w\"),\n",
    "#     max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = re.compile(r'([^ \\\\t]*)\\\\t*')\n",
    "\n",
    "def create_output(type_set, filename):\n",
    "    if type_set == 'test':\n",
    "        command = \"trec_eval -m all_trec -q ap_88_89/qrel_test \"\n",
    "    else:\n",
    "        command = \"trec_eval -m all_trec -q ap_88_89/qrel_validation \"\n",
    "    command +=  \"runfiles/\" + filename #+\" | grep -E '\\sall\\s'\"\n",
    "    \n",
    "    output = str(subprocess.check_output(command, shell = True))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_output(output, title):\n",
    "    # NDCG@10, Mean Average Precision (MAP@1000), Precision@5 and Recall@1000.\n",
    "    measure_results = {}\n",
    "    measures = [\"ndcg_cut_10\", [\"100\"]], [\"map_cut_1000\",[]], [\"P_5\", [\"500\", \"relative\"]], [\"recall_1000\",[]]\n",
    "    for measure in measures:\n",
    "        measure_list = []\n",
    "        measure_all = 0\n",
    "        for line in output.split():\n",
    "            if measure[0] in line:\n",
    "                clean = True\n",
    "                for restriction in measure[1]:\n",
    "                    if restriction in line:\n",
    "                        clean = False\n",
    "                if clean:\n",
    "                    if \"tall\" in line:\n",
    "                        measure_all = r.findall(line)[-1]\n",
    "                    else:\n",
    "                        measure_list.append(float(r.findall(line)[-1]))\n",
    "        measure_results[measure[0]] = measure_all, measure_list\n",
    "\n",
    "    return [title, measure_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all performance metrics of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = [\"ndcg_cut_10\", \"map_cut_1000\", \"P_5\",\"recall_1000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFidf\n",
      "BM25\n",
      "Jelinek lamb = 0.2\n",
      "Dirichlet mu = 2000\n",
      "AD delta = 0.8\n",
      "PLM\n",
      "Word2Vec\n",
      "LSI\n",
      "LDA\n",
      "Doc2Vec\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "outtf = create_output('test', 'TFIDFrun.run')\n",
    "restf = analyse_output(outtf, \"TFidf\")\n",
    "print(restf[0])\n",
    "tfidf = [[],[],[],[]]\n",
    "for key,value in restf[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    tfidf[ind] = value[1]\n",
    "\n",
    "#BM-25\n",
    "outbm = create_output('test', 'BM25run.run')\n",
    "resbm = analyse_output(outbm, \"BM25\")\n",
    "print(resbm[0])\n",
    "bm25 = [[],[],[],[]]\n",
    "for key,value in resbm[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    bm25[ind] = value[1]\n",
    "    \n",
    "#Jelinek-Mercer\n",
    "outl02t = create_output('test', 'jelinek_scores_test.run')\n",
    "resl02t = analyse_output(outl02t, \"Jelinek lamb = 0.2\")\n",
    "print(resl02t[0])\n",
    "jelinek = [[],[],[],[]]\n",
    "for key,value in resl02t[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    jelinek[ind] = value[1]\n",
    "\n",
    "output2000t = create_output('test', 'dirichlet_scores_test.run')\n",
    "measure_results2000t = analyse_output(output2000t, \"Dirichlet mu = 2000\")\n",
    "print(measure_results2000t[0])\n",
    "dirichlet = [[],[],[],[]]\n",
    "for key,value in measure_results2000t[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    dirichlet[ind] = value[1]\n",
    "    \n",
    "#Absolute Discounting\n",
    "outd08t = create_output('test', 'AD_scores_test.run')\n",
    "resd08t = analyse_output(outd08t, \"AD delta = 0.8\")\n",
    "print(resd08t[0])\n",
    "ad = [[],[],[],[]]\n",
    "for key,value in resd08t[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    ad[ind] = value[1]\n",
    "    \n",
    "#Positional Language Model\n",
    "outplm = create_output('test', 'PLM_scores.run')\n",
    "resplm = analyse_output(outplm, \"PLM\")\n",
    "print(resplm[0])\n",
    "plm = [[],[],[],[]]\n",
    "for key,value in resplm[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    plm[ind] = value[1]\n",
    "\n",
    "#Word2Vec\n",
    "outw2v = create_output('test', 'test.run')\n",
    "resw2v = analyse_output(outw2v, \"Word2Vec\")\n",
    "print(resw2v[0])\n",
    "w2v = [[],[],[],[]]\n",
    "for key,value in resw2v[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    w2v[ind] = value[1]\n",
    "\n",
    "#LSI\n",
    "outlsi = create_output('test', 'tfidf_lsi_2.run')\n",
    "reslsi = analyse_output(outlsi, \"LSI\")\n",
    "print(reslsi[0])\n",
    "lsi = [[],[],[],[]]\n",
    "for key,value in reslsi[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    lsi[ind] = value[1]    \n",
    "\n",
    "#LDA    \n",
    "outlda = create_output('test', 'tfidf_lda.run')\n",
    "reslda = analyse_output(outlda, \"LDA\")\n",
    "print(reslda[0])\n",
    "lda = [[],[],[],[]]\n",
    "for key,value in reslda[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    lda[ind] = value[1] \n",
    "    \n",
    "#Doc2Vec\n",
    "outd2v = create_output('test', 'tfidf_doc2vec.run')\n",
    "resd2v = analyse_output(outd2v, \"Doc2Vec\")\n",
    "print(resd2v[0])\n",
    "d2v = [[],[],[],[]]\n",
    "for key,value in resd2v[1].items():\n",
    "    ind = metrics.index(key)\n",
    "    d2v[ind] = value[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get P-values of 2tailed T-test and check difference in mean for alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = [tfidf, bm25, jelinek, dirichlet, plm, ad, w2v, lsi, lda, d2v]\n",
    "methods = ['tf-idf','bm25','jelinek','dirichlet','plm','ad','w2v','lsi','lda','d2v']\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT sign: 1.0 \t\t tf-idf and bm25\n",
      "NOT sign: 0.987 \t\t tf-idf and jelinek\n",
      "NOT sign: 0.997 \t\t tf-idf and dirichlet\n",
      "NOT sign: 0.907 \t\t tf-idf and plm\n",
      "NOT sign: 0.999 \t\t tf-idf and ad\n",
      "significant: 0.047854 \t tf-idf and w2v\n",
      "NOT sign: 0.074 \t\t tf-idf and lsi\n",
      "significant: 0.045423 \t tf-idf and lda\n",
      "significant: 0.042768 \t tf-idf and d2v\n",
      "NOT sign: 1.0 \t\t bm25 and tf-idf\n",
      "NOT sign: 0.98 \t\t bm25 and jelinek\n",
      "NOT sign: 0.998 \t\t bm25 and dirichlet\n",
      "NOT sign: 0.898 \t\t bm25 and plm\n",
      "NOT sign: 0.993 \t\t bm25 and ad\n",
      "significant: 0.035327 \t bm25 and w2v\n",
      "NOT sign: 0.067 \t\t bm25 and lsi\n",
      "significant: 0.032871 \t bm25 and lda\n",
      "significant: 0.030308 \t bm25 and d2v\n",
      "NOT sign: 0.987 \t\t jelinek and tf-idf\n",
      "NOT sign: 0.98 \t\t jelinek and bm25\n",
      "NOT sign: 0.964 \t\t jelinek and dirichlet\n",
      "NOT sign: 0.999 \t\t jelinek and plm\n",
      "NOT sign: 0.999 \t\t jelinek and ad\n",
      "NOT sign: 0.05 \t\t jelinek and w2v\n",
      "NOT sign: 0.11 \t\t jelinek and lsi\n",
      "significant: 0.047417 \t jelinek and lda\n",
      "significant: 0.04453 \t jelinek and d2v\n",
      "NOT sign: 0.997 \t\t dirichlet and tf-idf\n",
      "NOT sign: 0.998 \t\t dirichlet and bm25\n",
      "NOT sign: 0.964 \t\t dirichlet and jelinek\n",
      "NOT sign: 0.999 \t\t dirichlet and plm\n",
      "NOT sign: 0.998 \t\t dirichlet and ad\n",
      "significant: 0.016363 \t dirichlet and w2v\n",
      "significant: 0.039137 \t dirichlet and lsi\n",
      "significant: 0.015585 \t dirichlet and lda\n",
      "significant: 0.014734 \t dirichlet and d2v\n",
      "NOT sign: 0.907 \t\t plm and tf-idf\n",
      "NOT sign: 0.898 \t\t plm and bm25\n",
      "NOT sign: 0.999 \t\t plm and jelinek\n",
      "NOT sign: 0.999 \t\t plm and dirichlet\n",
      "NOT sign: 0.992 \t\t plm and ad\n",
      "significant: 0.013352 \t plm and w2v\n",
      "NOT sign: 0.084 \t\t plm and lsi\n",
      "significant: 0.012673 \t plm and lda\n",
      "significant: 0.011936 \t plm and d2v\n",
      "NOT sign: 0.999 \t\t ad and tf-idf\n",
      "NOT sign: 0.993 \t\t ad and bm25\n",
      "NOT sign: 0.999 \t\t ad and jelinek\n",
      "NOT sign: 0.998 \t\t ad and dirichlet\n",
      "NOT sign: 0.992 \t\t ad and plm\n",
      "significant: 0.048735 \t ad and w2v\n",
      "NOT sign: 0.095 \t\t ad and lsi\n",
      "significant: 0.046329 \t ad and lda\n",
      "significant: 0.043693 \t ad and d2v\n",
      "significant: 0.047854 \t w2v and tf-idf\n",
      "significant: 0.035327 \t w2v and bm25\n",
      "NOT sign: 0.05 \t\t w2v and jelinek\n",
      "significant: 0.016363 \t w2v and dirichlet\n",
      "significant: 0.013352 \t w2v and plm\n",
      "significant: 0.048735 \t w2v and ad\n",
      "NOT sign: 0.644 \t\t w2v and lsi\n",
      "NOT sign: 0.994 \t\t w2v and lda\n",
      "NOT sign: 0.987 \t\t w2v and d2v\n",
      "NOT sign: 0.074 \t\t lsi and tf-idf\n",
      "NOT sign: 0.067 \t\t lsi and bm25\n",
      "NOT sign: 0.11 \t\t lsi and jelinek\n",
      "significant: 0.039137 \t lsi and dirichlet\n",
      "NOT sign: 0.084 \t\t lsi and plm\n",
      "NOT sign: 0.095 \t\t lsi and ad\n",
      "NOT sign: 0.644 \t\t lsi and w2v\n",
      "NOT sign: 0.59 \t\t lsi and lda\n",
      "NOT sign: 0.529 \t\t lsi and d2v\n",
      "significant: 0.045423 \t lda and tf-idf\n",
      "significant: 0.032871 \t lda and bm25\n",
      "significant: 0.047417 \t lda and jelinek\n",
      "significant: 0.015585 \t lda and dirichlet\n",
      "significant: 0.012673 \t lda and plm\n",
      "significant: 0.046329 \t lda and ad\n",
      "NOT sign: 0.994 \t\t lda and w2v\n",
      "NOT sign: 0.59 \t\t lda and lsi\n",
      "NOT sign: 0.989 \t\t lda and d2v\n",
      "significant: 0.042768 \t d2v and tf-idf\n",
      "significant: 0.030308 \t d2v and bm25\n",
      "significant: 0.04453 \t d2v and jelinek\n",
      "significant: 0.014734 \t d2v and dirichlet\n",
      "significant: 0.011936 \t d2v and plm\n",
      "significant: 0.043693 \t d2v and ad\n",
      "NOT sign: 0.987 \t\t d2v and w2v\n",
      "NOT sign: 0.529 \t\t d2v and lsi\n",
      "NOT sign: 0.989 \t\t d2v and lda\n"
     ]
    }
   ],
   "source": [
    "for method1 in range(len(methods)):\n",
    "    for method2 in range(len(methods)):\n",
    "        if method1 != method2:\n",
    "            pvalues = []\n",
    "            for metric in range(len(metrics)):\n",
    "                \n",
    "                list_a = results[method1][metric]\n",
    "                list_b = results[method2][metric]\n",
    "                pvalue = scipy.stats.ttest_ind(list_a, list_b)[-1]\n",
    "                pvalues.append(pvalue)\n",
    "            \n",
    "            product = 1\n",
    "            for p in pvalues:\n",
    "                product *= (1-p)\n",
    "            new_p = 1-product\n",
    "            if new_p < alpha:\n",
    "                print('significant:', round(new_p,6), '\\t',methods[method1],'and', methods[method2])\n",
    "            else:\n",
    "                print('NOT sign:', round(new_p,3), '\\t\\t', methods[method1], 'and', methods[method2])\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
